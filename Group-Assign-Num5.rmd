---
title: "HW 4 - Tyson Brost, Chase Hatch, Isaac Palmer, Rachel Robertson"
output: 
  html_document:
    theme: cerulean
    code_folding: hide
---

<br>

```{r message=FALSE, warning=FALSE, include=FALSE}
library(pander)
library(tidyverse)
library(readxl)
library(plotly)
library(reshape2)
library(stargazer)
library(mosaic)
IO_airfare <- read_excel("IO airfare.xls")
colnames(IO_airfare) <- c("year", "origin", "destin", "id", "dist", "passen", "fare", "bkmtshr", "ldist", "y98", "y99", "y00", "lfare","ldistsq", "concen", "lpassen")
IO_airfare$LargeShare <- case_when(IO_airfare$bkmtshr >= 0.75 ~ 1, IO_airfare$bkmtshr < 0.75 ~ 0 )
IO_airfare$y97 <- case_when(IO_airfare$year == 1997 ~ 1, IO_airfare$year != 1997 ~ 0 )
IO_airfare$distsq <- IO_airfare$dist^2
```

## {.tabset .tabset-pills .tabset-fade}

### Problem #1 {.tabset .tabset-pills .tabset-fade}


#### Regressions  {.tabset .tabset-pills .tabset-fade}

##### Regression from Previous assignment


$$
\underbrace{Y_i}_\text{fare} \underbrace{=}_{\sim} \overbrace{\beta_0}^{\stackrel{\text{y-int}}{\text{base fare}}} + \overbrace{\beta_1}^{\stackrel{\text{slope}}{\text{baseline}}} \underbrace{X_{1i}}_\text{ldistance} + \overbrace{\beta_2}^{\stackrel{\text{change in}}{\text{y-int}}}  \underbrace{X_{2i}}_\text{lpassen} + \overbrace{\beta_3}^{\stackrel{\text{change in}}{\text{slope}}} \underbrace{X_{1i}X_{2i}}_\text{ldist:lpassen} + \epsilon_i
$$

```{r}
lm.mult <-lm(lfare ~ ldist + lpassen + ldist:lpassen, data=IO_airfare)
summary(lm.mult) %>%
pander(caption= "HW 4 Simple Multiple regression w/o extra estimators")
```

$$
\underbrace{Y_i}_\text{lfare} \underbrace{=}_{\sim} \overbrace{8.074}^{\stackrel{\text{y-int}}{\text{base lfare}}} + \overbrace{-0.3854}^{\stackrel{\text{slope}}{\text{baseline}}} \underbrace{X_{1i}}_\text{ldistance} + \overbrace{-0.9208}^{\stackrel{\text{change in}}{\text{y-int}}}  \underbrace{X_{2i}}_\text{lpassen} + \overbrace{0.1277}^{\stackrel{\text{change in}}{\text{slope}}} \underbrace{X_{1i}X_{2i}}_\text{ldist:lpassen} + \epsilon_i
$$

##### Regression W/ 2 interactions

Interpret your regression equation and explain what these two new interaction terms tell you about your dependent variable.


```{r}
lm.mult2 <-lm(fare ~ dist + passen + dist:passen + LargeShare + LargeShare:passen, data=IO_airfare)
summary(lm.mult2) %>%
pander(caption= "HW 4 Multiple regression results w/ extra estimators")
```

$$
\underbrace{Y_i}_\text{fare} \underbrace{=}_{\sim} \overbrace{\beta_0}^{\stackrel{\text{y-int}}{\text{base fare}}} + \overbrace{\beta_1}^{\stackrel{\text{slope}}{\text{baseline}}} \underbrace{X_{1i}}_\text{ldistance} + \overbrace{\beta_2}^{\stackrel{\text{change in}}{\text{y-int}}}  \underbrace{X_{2i}}_\text{lpassen} +  \overbrace{\beta_3}^{\stackrel{\text{change in}}{\text{slope}}} \underbrace{X_{1i}X_{2i}}_\text{ldist:lpassen}  + \overbrace{\beta_4}^{\stackrel{\text{change in}}{\text{y-int}}}  \underbrace{X_{4i}}_\text{LargeShare} + \overbrace{\beta_5}^{\stackrel{\text{change in}}{\text{y-int}}}  \underbrace{X_{5i}}_\text{y00} +  \overbrace{\beta_6}^{\stackrel{\text{change in}}{\text{y-int}}}  \underbrace{X_{6i}}_\text{y97} +\epsilon_i
$$


##### + Squared term

```{r}
lm.mult3 <-lm(fare ~ distsq + passen + dist:passen + LargeShare + LargeShare:passen, data=IO_airfare)
summary(lm.mult3) %>%
pander(caption= "HW 4 Multiple regression results w/ extra estimators")
```

Interpret your regression equation and explain what this new squared term tells you about the dependent variable.





##### + Ln's

```{r}
lm.mult4 <-lm(fare ~ ldistsq + passen + ldist:passen + LargeShare + LargeShare:passen, data=IO_airfare)
summary(lm.mult4) %>%
pander(caption= "HW 4 Multiple regression results w/ extra estimators")
```

 This will create a log-lin model or a lin-log model.  Interpret the findings with this new variable.  What does it mean?

##### Best Regression

Which of these three (new regressions) is the best equation to predict your dependent variable?  In comparing your three regression equations you should make sure to include the necessary evaluation techniques learned from chapter 2. 

The best of the three new regressions is the first, with the addition of 2 interaction terms, This equations R^2 still falls short of our previous assignment and both its regressions though.

##### Confidence Intervals

```{r}
confint(lm.mult2, level = 0.95) %>%
pander(caption= "HW 5 Estimators 95% Conf Int's")
```

Assuming that our decision to reject the null Hypothesis is correct then these 95% intervals capture the true coefficient values for each estimator 95% of the time given all possible results of our sample space. The most important result from these values is that non of the ranges include 0, so we can safely assume that each of these values are not equal to 0 at a 95% confidence level.


##### Completed Regression Equation

Here is the Base equation for the regression w/o Extra variables



Here is the original equation for the regression with the appropriate coefficients now included.

$$
\underbrace{Y_i}_\text{lfare} \underbrace{=}_{\sim} \overbrace{8.074}^{\stackrel{\text{y-int}}{\text{base lfare}}} + \overbrace{-0.3854}^{\stackrel{\text{slope}}{\text{baseline}}} \underbrace{X_{1i}}_\text{ldistance} + \overbrace{-0.9208}^{\stackrel{\text{change in}}{\text{y-int}}}  \underbrace{X_{2i}}_\text{lpassen} + \overbrace{0.1277}^{\stackrel{\text{change in}}{\text{slope}}} \underbrace{X_{1i}X_{2i}}_\text{ldist:lpassen} + \epsilon_i
$$



```{r eval=FALSE, include=FALSE}
library(rgl)
knitr::knit_hooks$set(webgl = hook_webgl)
```


#### Plot

A visualization of this regression as a whole would require a four dimensional plot. Decoding 4 dimensional images is so difficult it would likely add very little to the tabular outputs interpreted earlier, as such not plot will be created.

```{r eval=FALSE, message=FALSE, warning=FALSE, include=FALSE}
#b <- coef(lm.mult)
## Hint: library(car) has a scatterplot 3d function which is simple to use
#  but the code should only be run in your console, not knit.

library(car)
#scatter3d(fare ~ dist + passen, data=IO_airfare)



## To embed the 3d-scatterplot inside of your html document is harder.


#Perform the multiple regression

#Graph Resolution (more important for more complex shapes)
graph_reso <- 0.5

#Setup Axis
axis_x <- seq(min(IO_airfare$ldist), max(IO_airfare$ldist), by = graph_reso)
axis_y <- seq(min(IO_airfare$lpassen), max(IO_airfare$lpassen), by = graph_reso)

#Sample points
lmnew <- expand.grid(ldist = axis_x, lpassen = axis_y, KEEP.OUT.ATTRS=F)
lmnew$Z <- predict.lm(lm.mult2, newdata = lmnew)
lmnew <- acast(lmnew, lpassen ~ ldist, value.var = "Z") #y ~ x

#Create scatterplot
plot_ly(IO_airfare, 
        x = ~ldist, 
        y = ~lpassen, 
        z = ~lfare,
        text = rownames(IO_airfare), 
        type = "scatter3d", 
        mode = "markers", color=~lfare) %>%
  add_trace(z = lmnew,
            x = axis_x,
            y = axis_y,
            type = "surface")

  #add_trace(z = lmnew,
   #         x = axis_x,
    #        y = axis_y,
     #       type = "surface")
```
#### Interpretation/Assumptions {.tabset .tabset-pills .tabset-fade}

##### Interpretation 

Based on the multiple regression, the base cost of a ticket would be \$118.70, for each additional percentage increase in distance the fare would decrease by a percentage of 0.3852 and for each additional percent increase in average passengers the fare would decrease by a percentage of 0.9208. The strength or the relationship between Distance and passengers is ~0. The P-values for each of these terms are all incredibly close to 0.

These relationships are visible best when viewing the 3d plot. It is quickly apparent that all estimators have similarly weighted affects on the predicted values, as the points are spread evenly through the central area of the chart.. 

##### Assumptions

Assuming that our sample is random the following Q-Q plots aid in examining the residuals of our points. The first primarily helps to show if variance remains constant across our variables. The second shows some minor signs of right skewness, we agree this is likely due to the fact that the regression fails to predict base costs of a flight leaving these values up to B0. The third and final plot helps determine if the order of the data is important, usually this is needed for time sorted data but we noticed this set is sorted alphabetically by origin point so we included this to see if any patterns presented themselves. 

From these plots the primary change from the non-log version is that the extremes now show significantly less variance so the confidence in predictions for extremes may be greater.

```{r}
par(mfrow=c(1,3))
plot(lm.mult2,which=1:2)
plot(lm.mult2$residuals)
```

### Problem 2 

![ ](Hm-5.png)

#### Part A

Show that the return to another year of education, holding experience fixed is equal to $\beta 1 + \beta_3 (experience)$.

#### part B

State the null hypothesis that the return to education does not depend on the level of experience. What is the appropriate alternative hypothesis?


#### Part C

Use the data in Wage2 to test the null hypothesis against your stated alternative.

#### Part D

Let $\theta_1$ denote the return to education, when experience is 10 and $\theta_1 = \beta_1$ and a 95% confidence interval for for $\widehat{\theta_1}$. (hint: write $\beta_1 = \theta_1 - 10\beta_3$ and plug this into the equation and rearrange). This give the regression for obtaining the confidence interval for $\theta_1$.

##
